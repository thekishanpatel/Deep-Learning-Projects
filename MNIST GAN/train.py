# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fx_d3Ui57m0Vql9d6ecBJfwvX8cx0xPp

# **Import Libraries**
"""

from numpy import expand_dims
from numpy import zeros
from numpy import ones
from numpy import vstack
from numpy.random import randn
from numpy.random import randint
from keras.datasets.mnist import load_data
from keras.optimizers import Adam
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Reshape
from keras.layers import Flatten
from keras.layers import Conv2D
from keras.layers import Conv2DTranspose
from keras.layers import LeakyReLU
from keras.layers import Dropout
from matplotlib import pyplot
import sys
"""# **Import Datasets**"""

def loaddata():
  (xtrain, ytrain), (xtest, ytest) = load_data()
  X = expand_dims(xtrain, axis = -1)
  X = X.astype('float32')
  X = X / 255.0
  return X

"""# **Optimizer**"""

adam = Adam(lr = 0.0002, beta_1 = 0.5)

"""# **Create a Generator and a Discrimintor**"""

def cgenerator(latent_dim):
  generator = Sequential()
  # Layer 1
  n_nodes = 128*7*7
  generator.add(Dense(n_nodes, input_dim=latent_dim))
  generator.add(LeakyReLU(alpha=0.2))
  generator.add(Reshape((7, 7, 128)))
  # Layer 2
  generator.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))
  generator.add(LeakyReLU(alpha=0.2))
  # Layer 3
  generator.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))
  generator.add(LeakyReLU(alpha=0.2))
  generator.add(Conv2D(1, (7,7), activation='sigmoid', padding='same'))
  return generator

def cdiscriminator(inp_shape = (28, 28, 1)):
	discriminator = Sequential()
  # Layer 1
	discriminator.add(Conv2D(64, (3,3), strides=(2, 2), padding='same', input_shape=inp_shape))
	discriminator.add(LeakyReLU(alpha=0.2))
	discriminator.add(Dropout(0.4))
  # Layer 2
	discriminator.add(Conv2D(64, (3,3), strides=(2, 2), padding='same'))
	discriminator.add(LeakyReLU(alpha=0.2))
	discriminator.add(Dropout(0.4))
  # Layer 3
	discriminator.add(Flatten())
	discriminator.add(Dense(1, activation='sigmoid'))
	# compile model
	opt = Adam(lr=0.0002, beta_1=0.5)
	discriminator.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])
	return discriminator

"""# **Create GAN**"""

def cgan(generator, discriminator):
  discriminator.trainable = False
  gan = Sequential()
  gan.add(generator) # Add Generator
  gan.add(discriminator) # Add Discriminator
  gan.compile(loss='binary_crossentropy', optimizer=adam) # Compile
  return gan

"""# **Generating Real and Fake Samples**"""

def generate_real_samples(dataset, nsamples):
  xi = randint(0, dataset.shape[0], nsamples) # Selecting Random Instances
  X = dataset[xi] # Get Selected Samples
  Y = ones((nsamples, 1)) # Label Real Images
  return X, Y

def latent_points(latent_dim, nsamples):
  xinput = randn(latent_dim * nsamples) # Generating Samples in Latent Space
  xinput = xinput.reshape(nsamples, latent_dim) # Reshape
  return xinput

def generate_fake_samples(generator, latent_dim, nsamples):
  xinput = latent_points(latent_dim, nsamples) # Generating points in Latent Space
  X = generator.predict(xinput) # Generate Fake Samples
  Y = zeros((nsamples, 1)) # Label Fake Images 
  return X, Y

"""# **Function to Plot Images**"""

def plot_images(examples, epoch, n = 10):
  for i in range(n * n):
    pyplot.subplot(n, n, 1 + i)    # define subplot
    pyplot.axis('off') # turn off axis
    pyplot.imshow(examples[i, :, :, 0], cmap='gray_r') # plot raw pixel data

"""# **Summarize Performance**"""

def summarize(epoch, generator, discriminator, dataset, latent_dim, nsamples = 100):
  # Real Samples
  xreal, yreal = generate_real_samples(dataset, nsamples)
  _, acc_real = discriminator.evaluate(xreal, yreal, verbose = 0)
  # Fake Samokes
  xfake, yfake = generate_fake_samples(generator, latent_dim, nsamples)
  _, acc_fake = discriminator.evaluate(xfake, yfake, verbose = 0)
  # Summarize
  print(">\t Real Accuracy: {}% \t>\t Fake Accuracy: {}%".format(acc_real*100, acc_fake*100))
  plot_images(xfake, epoch)

"""# **Training GAN**"""

def training_gan(gen, disc, gan, dataset, latent_dim, nepochs, bs):
  bat_per_epoch = int(dataset.shape[0]/bs)
  half_batch = int(bs/2)
  for i in range(nepochs):
    for j in range(bat_per_epoch):
      xreal, yreal = generate_real_samples(dataset, half_batch) # Real Samples
      xfake, yfake = generate_fake_samples(gen, latent_dim, half_batch) # Fake Samples
      X, Y = vstack((xreal, xfake)), vstack((yreal, yfake))
      disc_loss, _ = disc.train_on_batch(X,Y) # Update Discriminator
      xgan = latent_points(latent_dim, bs)
      ygan = ones((bs, 1))
      gan_loss = gan.train_on_batch(xgan, ygan) # Update Generator via Discriminator's Error
      # Summarize Loss
      print('>%d, %d/%d, d=%.3f, g=%.3f' % (i+1, j+1, bat_per_epoch, disc_loss, gan_loss))
    if (i + 1) % 10 == 0: summarize(i, gen, disc, dataset, latent_dim)
  return gen

latent_dim = 100 # Size of Latent Space
disc = cdiscriminator() # Discriminator Model
gen = cgenerator(latent_dim) # Generator Model
gan = cgan(gen, disc) # Gan Model
dataset = loaddata() # Load MNIST Images
g = training_gan(gen, disc, gan, dataset, latent_dim, 500, 256)

g.save(sys.argv[1])
